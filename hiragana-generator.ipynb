{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86587c0b-a56f-49de-b7f9-75c0f86a13cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import math\n",
    "import pathlib\n",
    "from datetime import datetime\n",
    "\n",
    "# 3rd Party\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "142ff4d3-814d-47e2-9262-eb0db5eb1bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: NVIDIA GeForce GTX 1060 6GB\n"
     ]
    }
   ],
   "source": [
    "# Japanese compatible font\n",
    "plt.rcParams['font.sans-serif'] = \"Microsoft YaHei\" \n",
    "\n",
    "# Computation device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(f\"Using device: CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c5f113-636a-4609-96c7-10495fc5f797",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ef64f57-54a7-4d9b-9979-77838fb0d6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images):\n",
    "    plt.figure(figsize=(32, 32))\n",
    "    plt.imshow(torch.cat([\n",
    "        torch.cat([i for i in images.cpu()], dim=-1),\n",
    "    ], dim=-2).permute(1, 2, 0).cpu())\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_images(images, path, **kwargs):\n",
    "    grid = torchvision.utils.make_grid(images, **kwargs)\n",
    "    ndarr = grid.permute(1, 2, 0).to('cpu').numpy()\n",
    "    im = Image.fromarray(ndarr)\n",
    "    im.save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cae74b-9a18-4db5-b467-65abe057ec14",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bf7eb66-33c5-434c-9136-01543ed48ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = 'Kuzushiji-49'\n",
    "DATASET_TRAIN_X_FILE = 'k49-train-imgs.npz'\n",
    "DATASET_TRAIN_Y_FILE = 'k49-train-labels.npz'\n",
    "DATASET_TEST_X_FILE = 'k49-test-imgs.npz'\n",
    "DATASET_TEST_Y_FILE = 'k49-test-labels.npz'\n",
    "DATASET_CLASSMAP = 'k49_classmap.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4024c33a-2b4f-4cfb-89c4-0edc030c9778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: Input shape: torch.Size([232365, 28, 28]). Output shape: torch.Size([232365])\n",
      "Testing Set: Input shape: torch.Size([38547, 28, 28]). Output shape: torch.Size([38547])\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "train_images = torch.tensor(np.load(os.path.join(os.getcwd(), DATASET_DIR, DATASET_TRAIN_X_FILE))['arr_0'], dtype=torch.float32).to(device)\n",
    "train_labels = torch.tensor(np.load(os.path.join(os.getcwd(), DATASET_DIR, DATASET_TRAIN_Y_FILE))['arr_0'], dtype=torch.int64).to(device)\n",
    "test_images = torch.tensor(np.load(os.path.join(os.getcwd(), DATASET_DIR, DATASET_TEST_X_FILE))['arr_0'], dtype=torch.float32).to(device)\n",
    "test_labels = torch.tensor(np.load(os.path.join(os.getcwd(), DATASET_DIR, DATASET_TEST_Y_FILE))['arr_0'], dtype=torch.int64).to(device)\n",
    "print(f\"Training Set: Input shape: {train_images.shape}. Output shape: {train_labels.shape}\")\n",
    "print(f\"Testing Set: Input shape: {test_images.shape}. Output shape: {test_labels.shape}\")\n",
    "\n",
    "class_map = pd.read_csv(os.path.join(os.getcwd(), DATASET_DIR, DATASET_CLASSMAP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9c9d466-8647-46f2-a0ab-9c4594c0dede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels into one-hot encoder\n",
    "train_labels = torch.nn.functional.one_hot(train_labels, num_classes=49).to(torch.float32)\n",
    "test_labels = torch.nn.functional.one_hot(test_labels, num_classes=49).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feaf7af0-9ed6-4768-a871-7760dc56bab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add one dimension for channel\n",
    "train_images = train_images.unsqueeze(1)\n",
    "test_images = test_images.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22d815ee-16d4-416f-948f-809d6720a346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validation set\n",
    "validation_size = int(train_images.shape[0] * 0.1)\n",
    "validate_images = train_images[-validation_size:]\n",
    "train_images = train_images[:-validation_size]\n",
    "validate_labels = train_labels[-validation_size:]\n",
    "train_labels = train_labels[:-validation_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9affe66-bca6-4237-a750-a4be6d609326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform input images into 32x32\n",
    "pad2 = transforms.Pad(2)\n",
    "train_images = pad2(train_images)\n",
    "validate_images = pad2(validate_images)\n",
    "test_images = pad2(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "928fe3aa-86fa-4597-858c-5b43b1da39bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift pixel values into [-1,1]\n",
    "def shiftInputImage(images):\n",
    "    return (images-128)/128\n",
    "train_images = shiftInputImage(train_images)\n",
    "validate_images = shiftInputImage(validate_images)\n",
    "test_images = shiftInputImage(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4675efc3-f302-4c11-8aa8-cee532199509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discard test images: there are for classification purposes\n",
    "test_images = None\n",
    "test_labels = None\n",
    "# validate_images = None\n",
    "# validate_labels = None\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed77c6e2-d3fa-458d-a746-78f13a00006a",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e228703-36a9-414c-9900-880b84d6783c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion:\n",
    "    def __init__(self, noise_step=1000, beta_start=1e-4, beta_end=0.02, img_size=64):\n",
    "        self.noise_step = noise_step\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        self.img_size = img_size\n",
    "\n",
    "        self.beta = self.prepare_noise_schedule().to(device)\n",
    "        self.alpha = 1 - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "\n",
    "    def prepare_noise_schedule(self):\n",
    "        return torch.linspace(self.beta_start, self.beta_end, self.noise_step)\n",
    "\n",
    "    def noise_images(self, x, t):\n",
    "        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t]).reshape(t.shape[0], 1, 1, 1).repeat(1, 1, self.img_size, self.img_size)\n",
    "        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t]).reshape(t.shape[0], 1, 1, 1).repeat(1, 1, self.img_size, self.img_size)\n",
    "        Ɛ = torch.randn_like(x)\n",
    "        return (\n",
    "            (torch.mul(sqrt_alpha_hat, x)+torch.mul(sqrt_one_minus_alpha_hat,Ɛ)),\n",
    "            Ɛ\n",
    "        )\n",
    "\n",
    "    def sample_timesteps(self, n):\n",
    "        return torch.randint(low=1, high=self.noise_step, size=(n,))\n",
    "\n",
    "    # denoise process\n",
    "    def sample(self, model, n):\n",
    "        logging.info(f\"Sampling {n} new images ...\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            x = torch.randn((n, 1, self.img_size, self.img_size)).to(device)\n",
    "            # for i in tqdm( reversed(range(1, self.noise_step)), position=0):\n",
    "            for i in reversed(range(1, self.noise_step)):\n",
    "                t_tensor = (torch.ones(n) * i).long().to(device)\n",
    "                t = i\n",
    "                predicted_noise = model(x, t_tensor)\n",
    "                alpha = self.alpha[t]\n",
    "                alpha_hat = self.alpha_hat[t]\n",
    "                beta = self.beta[t]\n",
    "                if i > 1:\n",
    "                    noise = torch.randn_like(x)\n",
    "                else:\n",
    "                    # in the final iteration, we don't want to add noise back to X0\n",
    "                    noise = torch.zeros_like(x)\n",
    "                x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
    "\n",
    "        model.train()\n",
    "        x = (x.clamp(-1, 1) + 1) / 2\n",
    "        x = (x * 255).type(torch.uint8)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18694b3c-4086-41e6-92b2-0bc7b402e766",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, channels, size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.size = size\n",
    "        self.mha = nn.MultiheadAttention(channels, 4, batch_first=True)\n",
    "        self.ln = nn.LayerNorm([channels])\n",
    "        self.ff_self = nn.Sequential(\n",
    "            nn.LayerNorm([channels]),\n",
    "            nn.Linear(channels, channels),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(channels, channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.channels, self.size * self.size).swapaxes(1, 2)\n",
    "        x_ln = self.ln(x)\n",
    "        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n",
    "        attention_value = attention_value + x\n",
    "        attention_value = self.ff_self(attention_value) + attention_value\n",
    "        return attention_value.swapaxes(2, 1).view(-1, self.channels, self.size, self.size)\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, mid_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.residual:\n",
    "            return F.gelu(x + self.double_conv(x))\n",
    "        else:\n",
    "            return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=256):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, in_channels, residual=True),\n",
    "            DoubleConv(in_channels, out_channels),\n",
    "        )\n",
    "\n",
    "        self.emb_layer = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(\n",
    "                emb_dim,\n",
    "                out_channels\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        x = self.maxpool_conv(x)\n",
    "        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
    "        return x + emb\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.conv = nn.Sequential(\n",
    "            DoubleConv(in_channels, in_channels, residual=True),\n",
    "            DoubleConv(in_channels, out_channels, in_channels // 2),\n",
    "        )\n",
    "\n",
    "        self.emb_layer = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(\n",
    "                emb_dim,\n",
    "                out_channels\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip_x, t):\n",
    "        x = self.up(x)\n",
    "        x = torch.cat([skip_x, x], dim=1)\n",
    "        x = self.conv(x)\n",
    "        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
    "        return x + emb\n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    # sf: scaling factor of cnn kernel numbers\n",
    "    def __init__(self, c_in=1, c_out=1, time_dim=256, sf=16):\n",
    "        super().__init__()\n",
    "        self.time_dim = time_dim\n",
    "        self.inc = DoubleConv(c_in, sf*1)\n",
    "        self.down1 = Down(sf*1, sf*2)\n",
    "        self.sa1 = SelfAttention(sf*2, 16)\n",
    "        self.down2 = Down(sf*2, sf*4)\n",
    "        self.sa2 = SelfAttention(sf*4, 8)\n",
    "        self.down3 = Down(sf*4, sf*4)\n",
    "        self.sa3 = SelfAttention(sf*4, 4)\n",
    "\n",
    "        self.bot1 = DoubleConv(sf*4, sf*8)\n",
    "        self.bot2 = DoubleConv(sf*8, sf*8)\n",
    "        self.bot3 = DoubleConv(sf*8, sf*4)\n",
    "\n",
    "        self.up1 = Up(sf*4+sf*4, sf*2)\n",
    "        self.sa4 = SelfAttention(sf*2, 8)\n",
    "        self.up2 = Up(sf*2+sf*2, sf*1)\n",
    "        self.sa5 = SelfAttention(sf*1, 16)\n",
    "        self.up3 = Up(sf*1+sf*1, sf*1)\n",
    "        self.sa6 = SelfAttention(sf*1, 32)\n",
    "        self.outc = nn.Conv2d(sf*1, c_out, kernel_size=1)\n",
    "\n",
    "    def pos_encoding(self, t, channels):\n",
    "        inv_freq = 1.0 / (\n",
    "            10000\n",
    "            ** (torch.arange(0, channels, 2, device=device).float() / channels)\n",
    "        )\n",
    "        pos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n",
    "        return pos_enc\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t = t.unsqueeze(-1).type(torch.float)\n",
    "        t = self.pos_encoding(t, self.time_dim)\n",
    "\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1, t)\n",
    "        x2 = self.sa1(x2)\n",
    "        x3 = self.down2(x2, t)\n",
    "        x3 = self.sa2(x3)\n",
    "        x4 = self.down3(x3, t)\n",
    "        x4 = self.sa3(x4)\n",
    "\n",
    "        x4 = self.bot1(x4)\n",
    "        x4 = self.bot2(x4)\n",
    "        x4 = self.bot3(x4)\n",
    "\n",
    "        x = self.up1(x4, x3, t)\n",
    "        x = self.sa4(x)\n",
    "        x = self.up2(x, x2, t)\n",
    "        x = self.sa5(x)\n",
    "        x = self.up3(x, x1, t)\n",
    "        x = self.sa6(x)\n",
    "        output = self.outc(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b451588-81a2-4509-8535-1091b9d93e82",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76566804-b13e-4dae-b7c7-06d2a9a82f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs=500, batch_size=6, image_size=32, lr=3e-4, patience=5):\n",
    "    model = UNet(c_in=1, c_out=1, sf=8).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    mse = nn.MSELoss()\n",
    "    diffusion = Diffusion(img_size=image_size, noise_step=200)\n",
    "    training_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    pathlib.Path(f'results/{training_timestamp}').mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "    train_total_step = math.ceil(train_images.shape[0] / batch_size)\n",
    "    val_total_step = math.ceil(validate_images.shape[0] / batch_size)\n",
    "\n",
    "    prev_val_loss = math.inf\n",
    "    patience_left = patience\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Starting epoch {epoch}:\")\n",
    "        train_error = 0\n",
    "        model.train()\n",
    "        print(f\"Train:\")\n",
    "        pbar = tqdm(range(train_total_step))\n",
    "        for i in pbar:\n",
    "            optimizer.zero_grad()\n",
    "            if ((i+1)*batch_size > train_images.shape[0]):\n",
    "                images = train_images[i*batch_size:]\n",
    "            else:\n",
    "                images = train_images[i*batch_size:(i+1)*batch_size]\n",
    "            t = diffusion.sample_timesteps(images.shape[0]).to(device)\n",
    "            x_t, noise = diffusion.noise_images(images, t)\n",
    "            predicted_noise = model(x_t, t)\n",
    "            loss = mse(noise, predicted_noise)\n",
    "            train_error += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.set_postfix(MSE=(train_error/(i+1)))\n",
    "\n",
    "            if (epoch * train_total_step + i)%100 == 0:\n",
    "                sampled_images = diffusion.sample(model, n=16)\n",
    "                save_images(sampled_images, os.path.join(\"results\", f\"{training_timestamp}\", f\"{(epoch * train_total_step + i)}.jpg\"))\n",
    "        # torch.save(model.state_dict(), os.path.join(\"models\", f\"ckpt.pt\"))\n",
    "        avg_train_error = (train_error/train_total_step)\n",
    "                \n",
    "        # validate\n",
    "        model.eval()\n",
    "        print(f\"Validate:\")\n",
    "        val_error = 0\n",
    "        pbar = tqdm(range(val_total_step))\n",
    "        for i in pbar:\n",
    "            if ((i+1)*batch_size > validate_images.shape[0]):\n",
    "                images = validate_images[i*batch_size:]\n",
    "            else:\n",
    "                images = validate_images[i*batch_size:(i+1)*batch_size]\n",
    "            t = diffusion.sample_timesteps(images.shape[0]).to(device)\n",
    "            x_t, noise = diffusion.noise_images(images, t)\n",
    "            predicted_noise = model(x_t, t)\n",
    "            loss = mse(noise, predicted_noise)\n",
    "            val_error += loss.item()\n",
    "            pbar.set_postfix(MSE=(val_error/(i+1)))\n",
    "        avg_val_error = val_error/val_total_step\n",
    "\n",
    "        # early-stop control\n",
    "        if ((avg_val_error-prev_val_loss)>0.0001):\n",
    "            patience_left -= 1\n",
    "            print(f'Patience left: {patience_left}')\n",
    "            if patience < 0:\n",
    "                print(f'Early stopped')\n",
    "                break\n",
    "        else:\n",
    "            patience_left += 0.5\n",
    "            if patience_left > patience:\n",
    "                patience_left = patience\n",
    "        prev_val_loss = avg_val_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72b73674-39ab-4632-8c0b-9f73e9cb17aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0:\n",
      "Train:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 2615/2615 [21:18<00:00,  2.05it/s, MSE=0.126]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 291/291 [01:31<00:00,  3.17it/s, MSE=0.0806]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1:\n",
      "Train:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 2615/2615 [20:41<00:00,  2.11it/s, MSE=0.0749]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 291/291 [01:32<00:00,  3.14it/s, MSE=0.0721]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 2:\n",
      "Train:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 2615/2615 [20:37<00:00,  2.11it/s, MSE=0.069]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 291/291 [01:31<00:00,  3.17it/s, MSE=0.0666]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 3:\n",
      "Train:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 2615/2615 [20:48<00:00,  2.10it/s, MSE=0.066]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 291/291 [01:32<00:00,  3.16it/s, MSE=0.0646]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 4:\n",
      "Train:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 2615/2615 [21:31<00:00,  2.03it/s, MSE=0.0641]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 291/291 [01:41<00:00,  2.87it/s, MSE=0.0633]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 5:\n",
      "Train:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 2615/2615 [23:01<00:00,  1.89it/s, MSE=0.0626]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 291/291 [01:43<00:00,  2.82it/s, MSE=0.0615]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 6:\n",
      "Train:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 2615/2615 [22:19<00:00,  1.95it/s, MSE=0.0618]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 291/291 [01:36<00:00,  3.02it/s, MSE=0.061]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 7:\n",
      "Train:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 2615/2615 [21:05<00:00,  2.07it/s, MSE=0.0609]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 291/291 [01:32<00:00,  3.14it/s, MSE=0.061]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 8:\n",
      "Train:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 2615/2615 [20:58<00:00,  2.08it/s, MSE=0.0604]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 291/291 [01:32<00:00,  3.15it/s, MSE=0.0609]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 9:\n",
      "Train:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|███████████████████████████████████████████████████████▊            | 2147/2615 [17:16<03:45,  2.07it/s, MSE=0.06]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 26\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epochs, batch_size, image_size, lr, patience)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     images \u001b[38;5;241m=\u001b[39m train_images[i\u001b[38;5;241m*\u001b[39mbatch_size:(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mbatch_size]\n\u001b[1;32m---> 26\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[43mdiffusion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_timesteps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m x_t, noise \u001b[38;5;241m=\u001b[39m diffusion\u001b[38;5;241m.\u001b[39mnoise_images(images, t)\n\u001b[0;32m     28\u001b[0m predicted_noise \u001b[38;5;241m=\u001b[39m model(x_t, t)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(epochs=20, batch_size=80, image_size=32, lr=3e-4, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7877cec1-edae-4bf1-acba-13c94ca55d11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
